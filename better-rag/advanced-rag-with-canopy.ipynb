{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d3573c80a7114226bb414751494a68b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2a46132cb401493784cfc6b3ad3c794d",
              "IPY_MODEL_d3aa981837014c33b484767043966f4b",
              "IPY_MODEL_ec9cd65f64264b12a7a275f8d26eb86c"
            ],
            "layout": "IPY_MODEL_cbe2e7ad44124a5b917c1f8bb2cc7db9"
          }
        },
        "2a46132cb401493784cfc6b3ad3c794d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_babf021aafb4453783641c6fae1616c1",
            "placeholder": "​",
            "style": "IPY_MODEL_6583ed72196c4efa843564a2ea980fca",
            "value": "100%"
          }
        },
        "d3aa981837014c33b484767043966f4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd733b844ca246d2968fb4db085364fa",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9ebedfb13bb049f5bab139cab603da64",
            "value": 6
          }
        },
        "ec9cd65f64264b12a7a275f8d26eb86c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00e763e75e714ba0b2c171319b252d97",
            "placeholder": "​",
            "style": "IPY_MODEL_449f49df34624a79a2191acd68889b69",
            "value": " 6/6 [00:14&lt;00:00,  2.30s/it]"
          }
        },
        "cbe2e7ad44124a5b917c1f8bb2cc7db9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "babf021aafb4453783641c6fae1616c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6583ed72196c4efa843564a2ea980fca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dd733b844ca246d2968fb4db085364fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ebedfb13bb049f5bab139cab603da64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "00e763e75e714ba0b2c171319b252d97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "449f49df34624a79a2191acd68889b69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced RAG\n",
        "\n",
        "We’ll use [Canopy](https://github.com/pinecone-io/canopy) to drive the embedding and querying process, and add additional processes around it. Namely, we’re going to do the following:\n",
        "\n",
        "1. Query expansion: we’ll generate additional queries based on the query the user submits, in order to increase the chances of retrieving as many relevant documents as possible.\n",
        "2. Critique: We’ll create a function which uses the LLM to produce a critique of the content we retrieve.\n",
        "3. External search: We’ll create a “search tool” that our system can use in cases where the content retrieved doesn’t fall within a certain critique threshold.\n",
        "4. Reranking: We’ll use the Cohere reranker to ensure the order of retrieved documents is optimal.\n",
        "5. Comparing “naive” RAG to the “advanced” RAG methods."
      ],
      "metadata": {
        "id": "av7VkwuYx-m_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install dependencies"
      ],
      "metadata": {
        "id": "y4sj2Perx6AM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvWYCwQUykz_"
      },
      "outputs": [],
      "source": [
        "!pip install -qU canopy-sdk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain langchain_openai cohere==4.27 markdown google-search-results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-I26-til5txw",
        "outputId": "4e93e371-f4ad-4a9b-aa2b-c5170ced64f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fix for Canopy on colab"
      ],
      "metadata": {
        "id": "x_mLmRUA_Ms_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y numpy\n",
        "!pip install numpy==1.24.4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "id": "gjKnxGmEYnAX",
        "outputId": "52153c85-7ade-45ff-b9ea-2cd14d6701c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 1.26.4\n",
            "Uninstalling numpy-1.26.4:\n",
            "  Successfully uninstalled numpy-1.26.4\n",
            "Collecting numpy==1.24.4\n",
            "  Downloading numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pandas-stubs 2.2.1.240316 requires numpy>=1.26.0; python_version < \"3.13\", but you have numpy 1.24.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.24.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "3d380ac5987e44c4810599babe3c0efc"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Credentials"
      ],
      "metadata": {
        "id": "qE_zfXrax0Hu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "print(\"Provide your Pinecone key\")\n",
        "os.environ[\"PINECONE_API_KEY\"] =  getpass.getpass()\n",
        "print(\"Provide your OpenAI API key\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
        "print(\"Provide your Serp API key\")\n",
        "os.environ[\"SERPAPI_API_KEY\"] = getpass.getpass()\n",
        "print(\"Provide your Cohere API key\")\n",
        "os.environ[\"COHERE_API_KEY\"] = getpass.getpass()"
      ],
      "metadata": {
        "id": "wXPY1HuI0Eqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read data"
      ],
      "metadata": {
        "id": "Bc0kq5WryQ0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "data = pd.read_parquet(\"https://storage.googleapis.com/pinecone-datasets-dev/pinecone_docs_ada-002/raw/file1.parquet\")\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "uMSH2Ywf1efP",
        "outputId": "85d4bf3b-ac00-4dac-b449-07f2a621cdb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                     id   \n",
              "0  728aeea1-1dcf-5d0a-91f2-ecccd4dd4272  \\\n",
              "1  2f19f269-171f-5556-93f3-a2d7eabbe50f   \n",
              "2  b2a71cb3-5148-5090-86d5-7f4156edd7cf   \n",
              "3  1dafe68a-2e78-57f7-a97a-93e043462196   \n",
              "4  8b07b24d-4ec2-58a1-ac91-c8e6267b9ffd   \n",
              "\n",
              "                                                text   \n",
              "0  # Scale indexes\\n\\n[Suggest Edits](/edit/scali...  \\\n",
              "1  # Understanding organizations\\n\\n[Suggest Edit...   \n",
              "2  # Manage datasets\\n\\n[Suggest Edits](/edit/dat...   \n",
              "3  # Architecture\\n\\n[Suggest Edits](/edit/archit...   \n",
              "4  # Moving to production\\n\\n[Suggest Edits](/edi...   \n",
              "\n",
              "                                              source   \n",
              "0      https://docs.pinecone.io/docs/scaling-indexes  \\\n",
              "1        https://docs.pinecone.io/docs/organizations   \n",
              "2             https://docs.pinecone.io/docs/datasets   \n",
              "3         https://docs.pinecone.io/docs/architecture   \n",
              "4  https://docs.pinecone.io/docs/moving-to-produc...   \n",
              "\n",
              "                                            metadata  \n",
              "0  {'created_at': '2023_10_25', 'title': 'scaling...  \n",
              "1  {'created_at': '2023_10_25', 'title': 'organiz...  \n",
              "2  {'created_at': '2023_10_25', 'title': 'datasets'}  \n",
              "3  {'created_at': '2023_10_25', 'title': 'archite...  \n",
              "4  {'created_at': '2023_10_25', 'title': 'moving-...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ac7feb46-8a4e-4ad9-9549-cdf4d6226794\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>source</th>\n",
              "      <th>metadata</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>728aeea1-1dcf-5d0a-91f2-ecccd4dd4272</td>\n",
              "      <td># Scale indexes\\n\\n[Suggest Edits](/edit/scali...</td>\n",
              "      <td>https://docs.pinecone.io/docs/scaling-indexes</td>\n",
              "      <td>{'created_at': '2023_10_25', 'title': 'scaling...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2f19f269-171f-5556-93f3-a2d7eabbe50f</td>\n",
              "      <td># Understanding organizations\\n\\n[Suggest Edit...</td>\n",
              "      <td>https://docs.pinecone.io/docs/organizations</td>\n",
              "      <td>{'created_at': '2023_10_25', 'title': 'organiz...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>b2a71cb3-5148-5090-86d5-7f4156edd7cf</td>\n",
              "      <td># Manage datasets\\n\\n[Suggest Edits](/edit/dat...</td>\n",
              "      <td>https://docs.pinecone.io/docs/datasets</td>\n",
              "      <td>{'created_at': '2023_10_25', 'title': 'datasets'}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1dafe68a-2e78-57f7-a97a-93e043462196</td>\n",
              "      <td># Architecture\\n\\n[Suggest Edits](/edit/archit...</td>\n",
              "      <td>https://docs.pinecone.io/docs/architecture</td>\n",
              "      <td>{'created_at': '2023_10_25', 'title': 'archite...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8b07b24d-4ec2-58a1-ac91-c8e6267b9ffd</td>\n",
              "      <td># Moving to production\\n\\n[Suggest Edits](/edi...</td>\n",
              "      <td>https://docs.pinecone.io/docs/moving-to-produc...</td>\n",
              "      <td>{'created_at': '2023_10_25', 'title': 'moving-...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ac7feb46-8a4e-4ad9-9549-cdf4d6226794')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ac7feb46-8a4e-4ad9-9549-cdf4d6226794 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ac7feb46-8a4e-4ad9-9549-cdf4d6226794');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-0b8f5090-6c0d-426c-bad6-33e503cbfdd9\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0b8f5090-6c0d-426c-bad6-33e503cbfdd9')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-0b8f5090-6c0d-426c-bad6-33e503cbfdd9 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 60,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 60,\n        \"samples\": [\n          \"728aeea1-1dcf-5d0a-91f2-ecccd4dd4272\",\n          \"06609e7b-6caa-5c83-a185-5e165621e44c\",\n          \"085c267b-eaae-5dc7-9d16-21073755d9a5\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 60,\n        \"samples\": [\n          \"# Scale indexes\\n\\n[Suggest Edits](/edit/scaling-indexes)In this topic, we explain how you can scale your indexes horizontally and vertically.\\n\\n\\nProjects in the `gcp-starter` environment do not support the features referred to here, including pods, replicas, and collections.\\n\\n\\n## Vertical vs. horizontal scaling\\n\\n\\nIf you need to scale your environment to accommodate more vectors, you can modify your existing index to scale it vertically or create a new index and scale horizontally. This article will describe both methods and how to scale your index effectively. \\n\\n\\n## Vertical scaling\\n\\n\\nScaling vertically is fast and involves no downtime. This is a good choice when you can't pause upserts and must continue serving traffic. It also allows you to double your capacity instantly. However, there are some factors to consider.\\n\\n\\nBy [changing the pod size](manage-indexes#changing-pod-sizes), you can scale to x2, x4, and x8 pod sizes, which means you are doubling your capacity at each step. Moving up to a new capacity will effectively double the number of pods used at each step. If you need to scale by smaller increments, then consider horizontal scaling. \\n\\n\\nThe number of base pods you specify when you initially create the index is static and cannot be changed. For example, if you start with 10 pods of `p1.x1` and vertically scale to `p1.x2`, this equates to 20 pods worth of usage. Neither can you change pod types with vertical scaling. If you want to change your pod type while scaling, then horizontal scaling is the better option. \\n\\n\\nYou can only scale index sizes up and cannot scale them back down.\\n\\n\\nSee our learning center for more information on [vertical scaling](https://www.pinecone.io/learn/testing-p2-collections-scaling/#vertical-scaling-on-p1-and-s1).\\n\\n\\n## Horizontal scaling\\n\\n\\nThere are two approaches to horizontal scaling in Pinecone: adding pods and adding replicas. Adding pods increases all resources but requires a pause in upserts; adding replicas only increases throughput and requires no pause in upserts.\\n\\n\\n### Adding pods\\n\\n\\nAdding pods to an index increases all resources, including available capacity. Adding pods to an existing index is possible using our <collections> feature. A collection is an immutable snapshot of your index in time: a collection stores the data but not the original index definition.\\n\\n\\nWhen you [create an index from a collection](manage-indexes#create-an-index-from-a-collection), you define the new index configuration. This allows you to scale the base pod count horizontally without scaling vertically. The main advantage of this approach is that you can scale incrementally instead of doubling capacity as with vertical scaling. Also, you can redefine pod types if you are experimenting or if you need to use a different pod type, such asperformance-optimized pods or storage-optimized pods. Another advantage of this method is that you can change your [metadata configuration](manage-indexes#selective-metadata-indexing) to redefine metadata fields as indexed or stored-only. This is important when [tuning your index](performance-tuning) for the best throughput. \\n\\n\\nHere are the general steps to make a copy of your index and create a new index while changing the pod type, pod count, metadata configuration, replicas, and all typical parameters when creating a new collection: \\n\\n\\n1. Pause upserts.\\n2. Create a collection from the current index.\\n3. Create an index from the collection with new parameters.\\n4. Continue upserts to the newly created index. Note: the URL has likely changed.\\n5. Delete the old index if desired.\\n\\n\\n### Adding replicas\\n\\n\\nEach replica duplicates the resources and data in an index. This means that adding additional replicas increases the throughput of the index but not its capacity. However, adding replicas does not require downtime.\\n\\n\\nThroughput in terms of queries per second (QPS) scales linearly with the number of replicas per index.\\n\\n\\nTo add replicas, use the `configure_index` operation to [increase the number of replicas for your index](manage-indexes#replicas).\\n\\n\\n## Next steps\\n\\n\\n* See our learning center for more information on [vertical scaling](https://www.pinecone.io/learn/testing-p2-collections-scaling/#vertical-scaling-on-p1-and-s1).\\n* Learn more about <collections>.\\nUpdated 29 days ago \\n\\n\\n\\n---\\n\\n* [Table of Contents](#)\\n* + [Vertical vs. horizontal scaling](#vertical-vs-horizontal-scaling)\\n\\t+ [Vertical scaling](#vertical-scaling)\\n\\t+ [Horizontal scaling](#horizontal-scaling)\\n\\t\\t- [Adding pods](#adding-pods)\\n\\t\\t- [Adding replicas](#adding-replicas)\\n\\t+ [Next steps](#next-steps)\\n\",\n          \"# Understanding projects\\n\\n[Suggest Edits](/edit/projects)## Overview\\n\\n\\nThis document explains the concepts related to Pinecone projects.\\n\\n\\n## Projects contain indexes and users\\n\\n\\nEach Pinecone project contains a number of [indexes](/docs/indexes) and users. Only a user who belongs to the project can access the indexes in that project. Each project also has at least one project owner. All of the pods in a single project are located in a single environment. \\n\\n\\n## Project settings\\n\\n\\nWhen you create a new project, you can choose the **name**, **deployment environment**, and **pod limit**.\\n\\n\\n### Project environment\\n\\n\\nWhen creating a project, you must choose a cloud environment for the indexes in that project. Your project environment can affect your [pricing](https://pinecone.io/pricing). The following table lists the available cloud regions, the corresponding values of the `environment` parameter for the [init() operation](quickstart#2-get-and-verify-your-pinecone-api-key), and which billing tier has access to each environment:\\n\\n\\n\\n\\n| Cloud region | `environment` value | Tier availability |\\n| --- | --- | --- |\\n| GCP Starter (Iowa)\\\\* | gcp-starter | Starter |\\n| GCP US-West-1 Free (N. California) | us-west1-gcp-free | Starter |\\n| GCP Asia-Southeast-1 (Singapore) | asia-southeast1-gcp-free | Starter |\\n| GCP US-West-4 (Las Vegas) | us-west4-gcp-free | Starter |\\n| GCP US-West-1 (N. California) | us-west1-gcp | Standard / Enterprise |\\n| GCP US-Central-1 (Iowa) | us-central1-gcp | Standard / Enterprise |\\n| GCP US-West-4 (Las Vegas) | us-west4-gcp | Standard / Enterprise |\\n| GCP US-East-4 (Virginia) | us-east4-gcp | Standard / Enterprise |\\n| GCP northamerica-northeast-1 | northamerica-northeast1-gcp | Standard / Enterprise |\\n| GCP Asia-Northeast-1 (Japan) | asia-northeast1-gcp | Standard / Enterprise |\\n| GCP Asia-Southeast-1 (Singapore) | asia-southeast1-gcp | Standard / Enterprise |\\n| GCP US-East-1 (South Carolina) | us-east1-gcp | Standard / Enterprise |\\n| GCP EU-West-1 (Belgium) | eu-west1-gcp | Standard / Enterprise |\\n| GCP EU-West-4 (Netherlands) | eu-west4-gcp | Standard / Enterprise |\\n| AWS US-East-1 (Virginia) | us-east-1-aws | Standard / Enterprise |\\n| Azure East US (Virginia) | eastus-azure | Standard / Enterprise |\\n\\n\\n\\\\* This environment has unique features and limitations. See [`gcp-starter` environment](starter-environment) for more information.\\n\\n\\n [Contact us](http://www.pinecone.io/contact/) if you need a dedicated deployment in other regions.\\n\\n\\nThe environment cannot be changed after the project is created.\\n\\n\\n### Project pod limit\\n\\n\\nYou can set the maximum number of pods that can be used in total across all indexes in a project. Use this to control costs.\\n\\n\\nThe pod limit can be changed only by the project owner.\\n\\n\\n### Project roles\\n\\n\\nThere are two project roles: **Project owner** and **project member.** Table 1 below summarizes the permissions for each role.\\n\\n\\n**Table 1: Project roles and permissions**\\n\\n\\n\\n\\n| Project role | Permissions in organization |\\n| --- | --- |\\n| Project owner | Manage project members |\\n|  | Manage project API keys |\\n|  | Manage pod limits |\\n| Project member | Access API keys |\\n|  | Create indexes in project |\\n|  | Use indexes in project |\\n\\n\\n## API keys\\n\\n\\nEach Pinecone [project](projects) has one or more API keys. In order to [make calls to the Pinecone API](quickstart), a user must provide a valid API key for the relevant Pinecone project.\\n\\n\\nTo view the API key for your project, open the [Pinecone console](https://app.pinecone.io), select the project, and click **API Keys**.\\n\\n\\n## Project ID\\n\\n\\nEach Pinecone project has a project ID. This hexadecimal string appears as part of the URL for API calls. \\n\\n\\nTo find a project's ID, follow these steps:\\n\\n\\n1. Go to the [Pinecone console](https://app.pinecone.io).\\n2. In the upper-left corner, select your project.\\n3. Click **Indexes**.\\n4. Under the name of your indexes, find the index URL. For example:\\n\\n\\n`example-index-1e3g52e.svc.us-east1-gcp.pinecone.io`\\n\\n\\nThe portion of the index URL after the index name and before the dot is the project ID. \\n\\n\\nFor example, in the index URL `test-index-3e2f43f.svc.us-east1-gcp.pinecone.io`, the project ID is `3e2f43f`.\\n\\nUpdated 29 days ago \\n\\n\\n\\n---\\n\\n* [Table of Contents](#)\\n* + [Overview](#overview)\\n\\t+ [Projects contain indexes and users](#projects-contain-indexes-and-users)\\n\\t+ [Project settings](#project-settings)\\n\\t\\t- [Project environment](#project-environment)\\n\\t\\t- [Project pod limit](#project-pod-limit)\\n\\t\\t- [Project roles](#project-roles)\\n\\t+ [API keys](#api-keys)\\n\\t+ [Project ID](#project-id)\\n\",\n          \"# Monitoring your usage\\n\\n[Suggest Edits](/edit/monitoring-usage)This document describes how to monitor the usage and costs for your Pinecone organization through the Pinecone console.\\n\\n\\nTo view your Pinecone usage, you must be the [organization owner](organizations#organization-owners) for your organization. This feature is only available to organizations on the Standard or Enterprise plans.\\n\\n\\nTo view your usage through the Pinecone console, follow these steps:\\n\\n\\n1. Log in to the [Pinecone console](https://app.pinecone.io).\\n2. In the left menu, click **Organizations**.\\n3. Click the **USAGE** tab.\\n\\n\\nAll dates are given in UTC to match billing invoices.\\n\\nUpdated 29 days ago \\n\\n\\n\\n---\\n\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 60,\n        \"samples\": [\n          \"https://docs.pinecone.io/docs/scaling-indexes\",\n          \"https://docs.pinecone.io/docs/projects\",\n          \"https://docs.pinecone.io/docs/monitoring-usage\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"metadata\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize Canopy"
      ],
      "metadata": {
        "id": "a6XnAbuRyUke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from canopy.knowledge_base import KnowledgeBase\n",
        "from canopy.tokenizer import Tokenizer\n",
        "Tokenizer.initialize()\n",
        "\n",
        "INDEX_NAME = \"advanced-rag\"\n",
        "kb = KnowledgeBase(index_name=INDEX_NAME)\n"
      ],
      "metadata": {
        "id": "YFQVqn0x1oUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a Canopy index, if it doesn't exist."
      ],
      "metadata": {
        "id": "JGLmYDEwyaHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from canopy.knowledge_base import list_canopy_indexes\n",
        "if not any(name.endswith(INDEX_NAME) for name in list_canopy_indexes()):\n",
        "    kb.create_canopy_index()"
      ],
      "metadata": {
        "id": "SzD8QcXg1tBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert rows in `data` to an array of Canopy `Document`."
      ],
      "metadata": {
        "id": "70SVIAOiygS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from canopy.models.data_models import Document\n",
        "documents = [Document(**row) for _, row in data.iterrows()]"
      ],
      "metadata": {
        "id": "2d06oGvEAU5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upsert the documents to the Canopy knowledge base."
      ],
      "metadata": {
        "id": "-rlXtS_Wyt3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "kb = KnowledgeBase(index_name=INDEX_NAME)\n",
        "kb.connect()\n",
        "batch_size = 10\n",
        "\n",
        "for i in tqdm(range(0, len(documents), batch_size)):\n",
        "    kb.upsert(documents[i: i+batch_size])"
      ],
      "metadata": {
        "id": "ECcZYZ2PAbxI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "d3573c80a7114226bb414751494a68b2",
            "2a46132cb401493784cfc6b3ad3c794d",
            "d3aa981837014c33b484767043966f4b",
            "ec9cd65f64264b12a7a275f8d26eb86c",
            "cbe2e7ad44124a5b917c1f8bb2cc7db9",
            "babf021aafb4453783641c6fae1616c1",
            "6583ed72196c4efa843564a2ea980fca",
            "dd733b844ca246d2968fb4db085364fa",
            "9ebedfb13bb049f5bab139cab603da64",
            "00e763e75e714ba0b2c171319b252d97",
            "449f49df34624a79a2191acd68889b69"
          ]
        },
        "outputId": "9d4abb32-611b-4bb0-d527-1ff66d7e3254"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/6 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d3573c80a7114226bb414751494a68b2"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the Canopy knowledge base."
      ],
      "metadata": {
        "id": "D1fr52qjyy6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from canopy.models.data_models import Query\n",
        "\n",
        "results = kb.query([Query(text=\"p1 pod capacity\")])"
      ],
      "metadata": {
        "id": "5g_LbPAmaV12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "PRuZY0uZawwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize the Context and Chat engines"
      ],
      "metadata": {
        "id": "XNDvAGVdzDz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from canopy.context_engine import ContextEngine\n",
        "context_engine = ContextEngine(kb)"
      ],
      "metadata": {
        "id": "1U1MPvTcAZG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from canopy.chat_engine import ChatEngine\n",
        "chat_engine = ChatEngine(context_engine)"
      ],
      "metadata": {
        "id": "8Qxo0B0xedb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple\n",
        "from canopy.models.data_models import Messages, UserMessage, AssistantMessage\n",
        "\n",
        "def chat(new_message: str, history: Messages) -> Tuple[str, Messages]:\n",
        "    messages = history + [UserMessage(content=new_message)]\n",
        "    response = chat_engine.chat(messages)\n",
        "    assistant_response = response.choices[0].message.content\n",
        "    return assistant_response, messages + [AssistantMessage(content=assistant_response)]"
      ],
      "metadata": {
        "id": "iESlTrwFejQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import ast\n",
        "\n",
        "def str_to_json(s):\n",
        "    try:\n",
        "        # First, attempt to parse the string as JSON\n",
        "        return json.loads(s)\n",
        "    except json.JSONDecodeError:\n",
        "        # If it fails, assume the string might be a Python literal\n",
        "        try:\n",
        "            return ast.literal_eval(s)\n",
        "        except (ValueError, SyntaxError):\n",
        "            # Handle the case where parsing fails for both methods\n",
        "            print(\"Error: Input string is neither valid JSON nor a valid Python literal.\")\n",
        "            return None\n",
        "\n"
      ],
      "metadata": {
        "id": "X5gRXYTIx-kZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate with LLM\n",
        "Here we call on the LLM itself to produce different types of evaluation of generated text in relation to a prompt."
      ],
      "metadata": {
        "id": "K6qtBKW9-ZWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_with_llm(model, prompt, generated_text):\n",
        "    \"\"\"\n",
        "    Uses a Large Language Model (LLM) to evaluate generated text.\n",
        "\n",
        "    :param model: An instance of the LLM, ready to generate responses.\n",
        "    :param prompt: The original prompt given to the system.\n",
        "    :param generated_text: The text generated by the SELF-RAG system.\n",
        "    :return: A dictionary containing critique scores or assessments.\n",
        "    \"\"\"\n",
        "    evaluations = {}\n",
        "\n",
        "    # Template for creating evaluation queries\n",
        "    def create_evaluation_query(template, **kwargs):\n",
        "        query = ChatPromptTemplate.from_template(template)\n",
        "        chain = query | model\n",
        "        return float(chain.invoke(kwargs).content)\n",
        "\n",
        "    # Evaluate Relevance\n",
        "    relevance_template = \"Given the context provided by the following prompt: '{prompt}', please evaluate on a scale from 0 to 1, where 1 is highly relevant and 0 is not relevant at all, how relevant is this generated response: '{generated_text}'? Provide a numerical score only.\"\n",
        "    evaluations['relevance'] = create_evaluation_query(relevance_template, prompt=prompt, generated_text=generated_text)\n",
        "\n",
        "    # Evaluate Clarity\n",
        "    clarity_template = \"How clear and easily understandable is this text: '{generated_text}'? Rate its clarity on a scale from 0 to 1, where 1 indicates that the text is very clear and 0 indicates that the text is very unclear. Provide a numerical score only.\"\n",
        "    evaluations['clarity'] = create_evaluation_query(clarity_template, prompt=prompt, generated_text=generated_text)\n",
        "\n",
        "    # Evaluate Coherence\n",
        "    coherence_template = \"On a scale from 0 to 1, with 1 being highly coherent and 0 being not coherent at all, how well do the ideas in this generated text: '{generated_text}' flow together? Consider if the text makes logical sense as a whole. Provide a numerical score only.\"\n",
        "    evaluations['coherence'] = create_evaluation_query(coherence_template, prompt=prompt, generated_text=generated_text)\n",
        "\n",
        "    # Evaluate Detail and Exhaustiveness\n",
        "    detail_template = \"Assessing the detail and exhaustiveness relative to the prompt '{prompt}', how thoroughly does this generated text: '{generated_text}' cover the topic? Rate on a scale from 0 to 1, where 1 is very detailed and exhaustive, and 0 is not detailed at all. Provide a numerical score only.\"\n",
        "    evaluations['details'] = create_evaluation_query(detail_template, prompt=prompt, generated_text=generated_text)\n",
        "\n",
        "    # Evaluate Suitability as an Answer\n",
        "    suitability_template = \"Evaluate the suitability of this generated text: '{generated_text}' as an answer to the original prompt '{prompt}'. On a scale from 0 to 1, where 1 is a perfect answer and 0 is completely unsuitable, provide a numerical score only.\"\n",
        "    evaluations['suitability'] = create_evaluation_query(suitability_template, prompt=prompt, generated_text=generated_text)\n",
        "\n",
        "    return evaluations\n"
      ],
      "metadata": {
        "id": "pVBVdCvq0hz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Critique\n",
        "The critique function creates the evaluations and produces a weighted average based on configurable weights. In this way, we can change the impact each evaluation score has on the final critique score."
      ],
      "metadata": {
        "id": "BZRJQC_t-lGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def critique(model, prompt, generated_text):\n",
        "    evaluation_weights = {\n",
        "        'relevance': 3,\n",
        "        'clarity': 1,\n",
        "        'coherence': 0.5,\n",
        "        'details': 1.5,\n",
        "        'suitability': 2\n",
        "    }\n",
        "\n",
        "    evaluations = evaluate_with_llm(model, prompt, generated_text)\n",
        "    print(\"Evaluations:\", evaluations)\n",
        "\n",
        "    # Calculate the weighted sum of the evaluations\n",
        "    weighted_sum = sum(evaluations[aspect] * evaluation_weights.get(aspect, 1) for aspect in evaluations)\n",
        "\n",
        "    # Calculate the sum of weights for the aspects evaluated\n",
        "    total_weight = sum(evaluation_weights.get(aspect, 1) for aspect in evaluations)\n",
        "\n",
        "    # Calculate the weighted average of the evaluations\n",
        "    weighted_average = weighted_sum / total_weight if total_weight > 0 else 0\n",
        "\n",
        "    return [weighted_average, evaluations]\n"
      ],
      "metadata": {
        "id": "5oqAV4aE2gYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_retrieval_needed(model, prompt):\n",
        "  is_retrieval_needed_prompt = ChatPromptTemplate.from_template(\"Given the prompt: '{prompt}', is retrieval from an external source necessary to answer the question? Reply with only True or False\")\n",
        "  is_retrieval_needed_chain = is_retrieval_needed_prompt | model\n",
        "\n",
        "  return is_retrieval_needed_chain.invoke({\"prompt\": prompt}).content"
      ],
      "metadata": {
        "id": "6p23RHPIY55v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def consolidate(model, text):\n",
        "  consolidate_prompt = ChatPromptTemplate.from_template(\"Given the following set of texts, please consolidate them: '{text}'\")\n",
        "  consolidate_chain = consolidate_prompt | model\n",
        "\n",
        "  return consolidate_chain.invoke({\"text\": text}).content"
      ],
      "metadata": {
        "id": "Cpi42AhB3Uw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare(model, query, text1, text2):\n",
        "  compare_prompt = ChatPromptTemplate.from_template(\"Given the following query: '{query}', score text1 and text2 between 0 and 1, to indicate which provides a better answer overall to the query. Reply with two numbers in an array, for example: [0.1, 0.9]. The sum total of the values should be 1. text1: '{text1}' \\n text2: '{text2}'\")\n",
        "  compare_chain = compare_prompt | model\n",
        "\n",
        "  return str_to_json(compare_chain.invoke({\"query\": query, \"text1\": text1, \"text2\": text2}).content)"
      ],
      "metadata": {
        "id": "hz1g-imTa58O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_queries(model,prompt, num_queries):\n",
        "  query_generation_prompt = ChatPromptTemplate.from_template(\"Given the prompt: '{prompt}', generate {num_queries} questions that are better articulated. Return in the form of an list. For example: ['question 1', 'question 2', 'question 3']\")\n",
        "  query_generation_chain = query_generation_prompt | model\n",
        "  return str_to_json(query_generation_chain.invoke({\"prompt\": prompt, \"num_queries\": num_queries}).content)"
      ],
      "metadata": {
        "id": "KQ8oJFOQbHAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A helper function to get text from an array of Canopy `Document` objects."
      ],
      "metadata": {
        "id": "BHyWLQxwzcv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from canopy.models.data_models import Query\n",
        "\n",
        "def extract_documents_texts(results):\n",
        "    # Initialize an empty list to store the extracted texts\n",
        "    all_texts = []\n",
        "    # Loop through each QueryResult in the results list\n",
        "    for result in results:\n",
        "        # Assuming result.documents is the correct way to access documents in a QueryResult\n",
        "        for document in result.documents:\n",
        "            # Assuming document.text is the correct way to access the text of a DocumentWithScore\n",
        "            all_texts.append(document.text)\n",
        "    # Return the flat list of all texts\n",
        "    return all_texts\n",
        "\n"
      ],
      "metadata": {
        "id": "CWOh4lelmDN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A reranking function that queries the knowledge base, then reranks the results using Cohere's `rerank-english-v2.0` model."
      ],
      "metadata": {
        "id": "xd-2_LFBzkW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cohere\n",
        "from langchain_community.utilities import SerpAPIWrapper\n",
        "co = cohere.Client(os.environ[\"COHERE_API_KEY\"])\n",
        "\n",
        "\n",
        "def get_reranked_result(query, top_n=1):\n",
        "  matches = kb.query([Query(text=query)])\n",
        "  docs = extract_documents_texts(matches)\n",
        "  rerank_results = co.rerank(model=\"rerank-english-v2.0\", query=query, documents=docs, top_n=top_n)\n",
        "  texts = []\n",
        "  for rerank_result in rerank_results:\n",
        "      # Accessing the 'text' field in the document attribute of each RerankResult\n",
        "      text = rerank_result.document['text']\n",
        "      texts.append(text)\n",
        "  return texts"
      ],
      "metadata": {
        "id": "BzoVlvOlyhv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code bellow defines the `QueryProcessor` and `QueryDetail` classes which are used in the `advanced_rag_query` function. In this segment, the code\n",
        "\n",
        "- Defines the**`QueryDetail`** class to encapsulate details about a query, including the query itself, a list to hold content responses, a score and details for critique, and flags for whether retrieval and search operations are necessary.\n",
        "    - The **`__init__`** method initializes these attributes. **`query`** is required, while others like **`content`** and **`critique_details`** are set to their default values.\n",
        "    - **`add_response`** method decides whether retrieval is needed based on the model's response to the query, then adjusts attributes based on critique results, including whether a search is needed.\n",
        "    - **`search_and_add_results`** method is called if the critique score is low, indicating poor response quality. It performs a search and adds the results to the content list.\n",
        "- Defines a **`QueryProcessor`** class to handle a list of queries using a specified model and search mechanism.\n",
        "    - The **`__init__`** method sets up the processor with a model, search tool, and initializes **`QueryDetail`** objects for each query.\n",
        "    - **`process_queries`** method processes each query through **`add_response`**, consolidates responses if a search was needed, and recalculates critique scores and details.\n",
        "- The **`advanced_rag_query`** function creates a setup for processing queries using an unspecified model and a **`SerpAPIWrapper`** for searches. It generates an initial list of queries, processes them with **`QueryProcessor`**, and returns the processed queries."
      ],
      "metadata": {
        "id": "4BhTn6kV_GbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict, Any, Tuple\n",
        "from collections import defaultdict\n",
        "\n",
        "class QueryDetail:\n",
        "    def __init__(self, query: str):\n",
        "        self.query = query\n",
        "        self.content: List[str] = []\n",
        "        self.critique_score: float = 0.0\n",
        "        self.critique_details: Dict[str, Any] = {}\n",
        "        self.retrieval_needed: bool = False\n",
        "        self.search_needed: bool = False\n",
        "\n",
        "    def add_response(self, model, search) -> None:\n",
        "        \"\"\"Process the query to add response, handle retrieval and critique.\"\"\"\n",
        "        if is_retrieval_needed(model, self.query):\n",
        "            response = \" \".join(get_reranked_result(self.query, top_n=3))\n",
        "            self.retrieval_needed = True\n",
        "        else:\n",
        "            response = \"Some generated answer\"\n",
        "            self.retrieval_needed = False\n",
        "\n",
        "        self.content.append(response)\n",
        "\n",
        "        critique_score, critique_details = critique(model, self.query, response)\n",
        "        self.critique_score = critique_score\n",
        "        self.critique_details = critique_details\n",
        "        self.search_needed = critique_score < 0.5\n",
        "\n",
        "        if self.search_needed:\n",
        "            self.search_and_add_results(search)\n",
        "\n",
        "    def search_and_add_results(self, search) -> None:\n",
        "        \"\"\"Perform a search and process the results if critique score is low.\"\"\"\n",
        "        search_result_raw = search.run(self.query)\n",
        "        search_result = str_to_json(search_result_raw) or []\n",
        "        self.content.extend(search_result)\n",
        "\n",
        "class QueryProcessor:\n",
        "    def __init__(self, model, search, queries: List[str]):\n",
        "        self.model = model\n",
        "        self.search = search\n",
        "        self.queries = [QueryDetail(query) for query in queries]\n",
        "\n",
        "    def process_queries(self) -> List[QueryDetail]:\n",
        "        \"\"\"Process each query in the list.\"\"\"\n",
        "        for query_detail in self.queries:\n",
        "            query_detail.add_response(self.model, self.search)\n",
        "            if query_detail.search_needed:\n",
        "                consolidated_response = consolidate(self.model, query_detail.content)\n",
        "                query_detail.content = [consolidated_response]\n",
        "                critique_score, critique_details = critique(self.model, query_detail.query, consolidated_response)\n",
        "                query_detail.critique_score = critique_score\n",
        "                query_detail.critique_details = critique_details\n",
        "        return self.queries\n",
        "\n",
        "def advanced_rag_query(model, query: str, num_queries: int) -> List[QueryDetail]:\n",
        "    search = SerpAPIWrapper()\n",
        "    initial_queries = generate_queries(model, query, num_queries)[:num_queries]\n",
        "    query_processor = QueryProcessor(model, search, initial_queries)\n",
        "    processed_queries = query_processor.process_queries()\n",
        "    return processed_queries\n"
      ],
      "metadata": {
        "id": "pcXpLFf9w75K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How can I make a new Pinecone index?\""
      ],
      "metadata": {
        "id": "wUYPn898IGo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "model = ChatOpenAI(model=\"gpt-4\")"
      ],
      "metadata": {
        "id": "ZhqjVh8yzYFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = advanced_rag_query(model, query, 3)\n",
        "combined_content = \" \".join(content for result in results for content in result.content)\n",
        "advanced_rag_results = consolidate(model, combined_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gp98cMcj3_N5",
        "outputId": "4dfbf69c-95c3-4296-c04a-d1914a03981e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluations: {'relevance': 0.875, 'clarity': 0.8, 'coherence': 0.9, 'details': 0.8, 'suitability': 0.85}\n",
            "Evaluations: {'relevance': 1.0, 'clarity': 0.8, 'coherence': 0.8, 'details': 0.9, 'suitability': 0.8}\n",
            "Evaluations: {'relevance': 1.0, 'clarity': 0.8, 'coherence': 0.9, 'details': 0.9, 'suitability': 0.9}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "advanced_rag_critique = critique(model, query, advanced_rag_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwL_8IVtv7sn",
        "outputId": "5897e49a-ff1e-44e9-d6d6-4ad86bd8d88b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluations: {'relevance': 1.0, 'clarity': 0.85, 'coherence': 1.0, 'details': 0.8, 'suitability': 1.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = []\n",
        "rag_result, history = chat(query, history)\n",
        "rag_critique = critique(model, query, rag_result)\n"
      ],
      "metadata": {
        "id": "jVs_ReYcwO9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31c15ba0-f0c9-46ac-8477-4b2157604c39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluations: {'relevance': 1.0, 'clarity': 0.8, 'coherence': 1.0, 'details': 0.9, 'suitability': 1.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, HTML\n",
        "import markdown\n",
        "\n",
        "# Convert your Markdown content and critiques to strings, assuming they might not be strings already\n",
        "final_result_html = markdown.markdown(str(advanced_rag_results))\n",
        "rag_result_html = markdown.markdown(str(rag_result))\n",
        "advanced_rag_critique_html = markdown.markdown(str(advanced_rag_critique))\n",
        "rag_critique_html = markdown.markdown(str(rag_critique))\n",
        "\n",
        "# Construct HTML table with the pre-rendered HTML content for each cell\n",
        "html = f\"\"\"\n",
        "<table>\n",
        "<tr>\n",
        "    <th>Advanced RAG</th>\n",
        "    <th>RAG</th>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td>{final_result_html}</td>\n",
        "    <td>{rag_result_html}</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td>{advanced_rag_critique_html}</td>\n",
        "    <td>{rag_critique_html}</td>\n",
        "</tr>\n",
        "</table>\n",
        "\"\"\"\n",
        "\n",
        "# Display the HTML table\n",
        "display(HTML(html))\n"
      ],
      "metadata": {
        "id": "AajdOKllvmgi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "outputId": "72090457-eb9f-4e29-ab4c-c27940629ea4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<table>\n",
              "<tr>\n",
              "    <th>Advanced RAG</th>\n",
              "    <th>RAG</th>\n",
              "</tr>\n",
              "<tr>\n",
              "    <td><p>To create an index in Pinecone, first download a pre-embedded dataset from the <code>pinecone-datasets</code> library. This allows you to skip the embedding and preprocessing steps. </p>\n",
              "<p>```python\n",
              "import pinecone_datasets</p>\n",
              "<p>dataset = pinecone_datasets.load_dataset('wikipedia-simple-text-embedding-ada-002-100K')\n",
              "dataset.head()\n",
              "```</p>\n",
              "<p>After downloading the data, initialize your Pinecone environment and create your first index. You have the option to select a distance metric for your index. </p>\n",
              "<p><strong>Note</strong> - By default, all fields are indexed. To avoid redundant and costly indexing, pass an additional empty metadata_config parameter.</p>\n",
              "<p><code>python\n",
              "pinecone.create_index(\n",
              "        name=index_name_v1,\n",
              "        metric='cosine',\n",
              "        dimension=1536,\n",
              "        metadata_config={“indexed”:[]} \n",
              ")</code></p>\n",
              "<p>Before creating an index, ensure your Pinecone API key is set up. If the 'openai' index already exists, you can connect to it directly.</p>\n",
              "<p>```python\n",
              "import pinecone</p>\n",
              "<p>pinecone.init(\n",
              "    api_key=\"YOUR_API_KEY\",\n",
              "    environment=\"YOUR_ENV\"\n",
              ")</p>\n",
              "<p>if 'openai' not in pinecone.list_indexes():\n",
              "    pinecone.create_index('openai', dimension=len(embeds[0]))</p>\n",
              "<p>index = pinecone.Index('openai')\n",
              "```</p>\n",
              "<p>When preparing your project structure, consider creating separate projects for your development and production indexes. This allows you to test changes before deploying them to production. Ensure that you have properly configured user access to your production environment. </p>\n",
              "<p>Before moving your index to production, test that your index is returning accurate results in the context of your application. Consider identifying the appropriate metrics for evaluating your results.</p></td>\n",
              "    <td><p>To create a new Pinecone index, you can follow these general steps:</p>\n",
              "<ol>\n",
              "<li>Initialize a connection to Pinecone using your API key and environment.</li>\n",
              "<li>Check if the index already exists, and if not, create a new index with a specified dimension.</li>\n",
              "<li>Connect to the newly created index for further operations.</li>\n",
              "</ol>\n",
              "<p>You can also consider specifying additional configurations and choices like the distance metric and metadata fields during the index creation process.</p></td>\n",
              "</tr>\n",
              "<tr>\n",
              "    <td><p>[0.94375, {'relevance': 1.0, 'clarity': 0.85, 'coherence': 1.0, 'details': 0.8, 'suitability': 1.0}]</p></td>\n",
              "    <td><p>[0.95625, {'relevance': 1.0, 'clarity': 0.8, 'coherence': 1.0, 'details': 0.9, 'suitability': 1.0}]</p></td>\n",
              "</tr>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compare(model, query, advanced_rag_results, rag_result)"
      ],
      "metadata": {
        "id": "rpiljdF5wzRP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3285fcb0-ba65-4406-949d-33e246a38f6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9, 0.1]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    }
  ]
}